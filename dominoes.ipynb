{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d4d84df-c50a-4eb1-88b3-3ba6b1ef7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create simple system to operate a dominoes game\n",
    "\n",
    "# -- top level \"API\" for game --\n",
    "# inputs:\n",
    "# 1. number of players\n",
    "# 2. maximum number of dominoes\n",
    "# 3. rule for creating player agent, or preset list of agents\n",
    "# operation:\n",
    "# 1. distribute dominoes to each player randomly\n",
    "# 2. create turn list\n",
    "# 3. query players for their play (evaluate whether the play is valid and accept or reject it)\n",
    "# 4. identify winner, track score, initiate new round until 0/0 is finished\n",
    "\n",
    "# -- agent --\n",
    "# 1. one-hot of dominoes in hand\n",
    "# 2. one-hot of dominoes already played\n",
    "# 3. multi-length vector, one input for each other player, containing: 1) how many dominoes in their hand, 2) whether they have a penny up, 3) how many turns until they play\n",
    "# 4. number of turns until the agent plays\n",
    "# 5. one-hot of dominoes available to play on \n",
    "# 6. dominoe played on last turn? \n",
    "\n",
    "# -- simple agents --\n",
    "# 1. play random dominoe\n",
    "# 2. play highest dominoe\n",
    "# 3. play double-pair dominoe\n",
    "# 4. RL agent...\n",
    "\n",
    "# -- observation agents --\n",
    "# 1. I can train an independent RL model to predict the points at the end of each game from the current game state and run that independently. \n",
    "#    This would work by feeding in the full game state, then having the network predict the value on the next turn, and then learn from the last turn moving backwards (with uncertainty etc.)\n",
    "#    Then, I can teach this observer using games from crystallized agents, and update one agent to see how much better it performs than change. \n",
    "#    Additionally, this agent could maybe be integrated into the gameplay agents? \n",
    "# 2. Could train an RL model to predict what dominoes are in other agents hands based on what has been played, their line, and what is available...\n",
    "\n",
    "# -- ideas for flexibility in terms of number of players --\n",
    "# 1. the agent could process \"own line\", \"opponent line\", \"dummy line\" in an LSTM network in 3+ steps, where an additional one-hot input determines whose line is being valued. \n",
    "#    in this way, the LSTM network would learn to process opponent lines generally, and then just receive additional processing steps for valuation of each line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4ea59-2675-47a1-b1df-e14d325aec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do list\n",
    "# 230628 -- I've completed the basic mechanics of the game, it works, and I can run games and develop agents now.\n",
    "#\n",
    "# Next: - read & think about how to train an agent to be smarter...\n",
    "#       - I think I need an actor-critic model, where the critic is learning to predict outcomes based on game-state and action, and the actor is making decisions\n",
    "#       - This is gonna be a bit hard and I'll need to read some papers to implement it intelligently.\n",
    "#       - but...\n",
    "#       - The simplest way is to: build one single network\n",
    "#                                 \"remember\" the states and actions chosen throughout an entire hand\n",
    "#                                 then use the final score as a teaching signal that propagates down to every action\n",
    "\n",
    "# -- setup infinite gameplay so an agent can be initialized in a game once, and then learn for a very very long time... --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3d3d05f-41b1-4604-87a0-6b93fa2d1f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Ideas:\n",
    "# 1. Teach line-formation by penalizing actions based on whether there's another play available (can be within or across lines...)\n",
    "# 2. Teach score estimation by training network to predict points in each other hand, using the uncertain naive estimate of (pointsLeft/numberOfPlayers*dominoesPerHand)? \n",
    "# 3. NOTE: Simple FF networks are pretty bad at estimating current hand value... they don't asymptote to perfect error even though it's just a dot product.\n",
    "\n",
    "# Overall Ideas: \n",
    "# - The network has to take in the gamestate information as input and output a value function of each dominoe/location pairing (which is essentially the state-action table, or Q)\n",
    "# - The network can estimate a value from the gamestate information independently. The value should be the expected score at the end of the game. \n",
    "# - Each time the network has to play, it can compare it's expected score at the beginning of it's turn with the expected score at the beginning of it's last turn as the TD error. \n",
    "# - Presumably, the network will get better and better at predicting the final score as it learns from the end-game scenarios...\n",
    "# - Maybe I can pretrain it by using TrainingIdea#2 above, and it can learn how much to use it's own value estimation and the naive estimate? \n",
    "\n",
    "# -- Initial Push --\n",
    "# Build an LSTM network that learns to predict the value of dominoes in each players hand based on the gamestate information (it steps each players turn)\n",
    "# It is trained from omniscient information (the error function is the difference between the prediction and the true hand value)\n",
    "# Idea: once this LSTM network is pretrained, it will be used as an input to the agent network for temporal difference learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d743f019-bcc5-4c13-8aef-98e737e0747e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dominoesGameplay as dg\n",
    "import dominoesAgents as da\n",
    "import dominoesNetworks as dn\n",
    "import dominoesFunctions as df\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb3a9b-62f2-4af8-a60d-45fe50151266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e59bf-627a-43f7-9b7a-eaa1181a537a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d57f8-0f45-4db3-99da-12d7958b629c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4882843-a56a-43a4-b8fb-b4bab90feb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps: \n",
    "# 1. add game simulation engine to agents (or game?) to permit agents to sample gameState for a particular action\n",
    "# 2. then, use the argmax action of future value to move the game forward\n",
    "\n",
    "# -- also, now that it's training and gameplay time, it's probably a good idea to optimize the code now...\n",
    "# -- regarding next step 1: \n",
    "# ------ convert most of the code within \"doTurn()\" to a standalone function that accepts the gameState and the next play as the inputs, and returns an updated gameState\n",
    "# ------ then, this function can be fed into the agents and they can simulate what their turn options would do...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f0dafaf7-668f-4632-85dc-8a0dc48a685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numPlayers = 4\n",
    "highestDominoe = 9\n",
    "numRounds = 1000\n",
    "game = dg.dominoeGame(highestDominoe, numPlayers=numPlayers, defaultAgent=da.valueAgent0, device=device)\n",
    "# game.playGame(rounds=numRounds,withUpdates=True)\n",
    "game.initializeHand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c9373aaa-64fb-4017-abab-84f0fb7def26",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.doTurn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "92d13f7c-623c-4ec9-8ae3-3dc85c2fb0bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'game' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [149]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m cfn \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m19\u001b[39m\n\u001b[0;32m      2\u001b[0m convFilter \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(cfn)\u001b[38;5;241m/\u001b[39mcfn\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgame\u001b[49m\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39mconvolve(np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mabs(agent\u001b[38;5;241m.\u001b[39mstoreSelfHandvalueError)),convFilter,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m),linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'game' is not defined"
     ]
    }
   ],
   "source": [
    "cfn = 19\n",
    "convFilter = np.ones(cfn)/cfn\n",
    "for agent in game.agents:\n",
    "    plt.plot(np.convolve(np.array(np.abs(agent.storeSelfHandvalueError)),convFilter,mode='same'),linewidth=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc533a9-e70d-472d-8712-df806ac96a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80945895-4eb3-4a57-8b1d-4cd0d51914c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110d803-fad8-49ae-a7d3-63aa163f445e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "426deffe-d870-4e63-9dea-c0cd14c2e9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "[4 0 1 2 3]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.eye(5)\n",
    "print(x)\n",
    "\n",
    "idxNextPlayerShift = np.mod(np.arange(5)-1,5)\n",
    "print(idxNextPlayerShift)\n",
    "\n",
    "print(x[idxNextPlayerShift])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc43f3-d3b9-40be-81e9-39d328056420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb6f563-cec7-4331-ae03-0d5df02f48d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911a238-16d3-4971-af72-7ab00008bc0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "541c580c-f2cd-4160-90eb-f41ac05ef26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "5.336737632751465e-05\n",
      "1.302783411681236e-05\n",
      "3.3613891042093196e-05\n",
      "1.0002245380520331e-05\n"
     ]
    }
   ],
   "source": [
    "# print performance monitoring\n",
    "print(game.prepareScoreTime[0]/game.prepareScoreTime[1])\n",
    "print(game.initHandTime[0]/game.initHandTime[1])\n",
    "print(game.presentGameStateTime[0]/game.presentGameStateTime[1])\n",
    "print(game.agentPlayTime[0]/game.agentPlayTime[1])\n",
    "print(game.processPlayTime[0]/game.processPlayTime[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d5e5c-ab44-40f1-a372-305b21a7a2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e51acf-050b-40ca-933c-f3501d62d463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8bed6-c877-46a1-a1fc-919648ef1b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a673e6-7f82-470c-aa1d-1d0534b7f402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1c58c-fec0-4da5-85bd-c782d7363c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dfcbf7-ef10-4254-a31d-e9d82eeb62ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c05474-de04-44da-b64d-25f2ead02ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea6089-01fc-43f8-8ca6-a58285390be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c6782-1de5-44bc-b9ef-187ba9eedc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62849fb-452a-4961-913e-d18460c613db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2d7e3-67db-43e5-9dad-f2479ec485fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f96572-bba5-486a-b677-3204e117b941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06a8e42f-29d5-4a18-830b-550e311a48e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [00:11<00:00, 25.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[216.  74.   9.   1.]\n",
      "[ 7.36962963 10.34148148 17.77111111 24.62037037]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "numPlayers = 4\n",
    "highestDominoe = 9\n",
    "winnerCount = np.zeros(numPlayers)\n",
    "scoreTally = np.zeros(numPlayers)\n",
    "numGames = 300\n",
    "numRounds = 9\n",
    "game = dg.dominoeGame(highestDominoe, agents=(da.doubleAgent, da.greedyAgent, None, da.stupidAgent))\n",
    "for _ in tqdm(range(numGames)):\n",
    "    game.playGame(rounds=numRounds)\n",
    "    # game.printResults()\n",
    "    winnerCount[game.currentWinner]+=1\n",
    "    scoreTally += game.currentScore\n",
    "print(winnerCount)\n",
    "print(scoreTally / numGames / numRounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfca2fc-5bd1-463a-ade8-df46ce91c895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game has already finished\n",
      "[[ 31  17  20   0]\n",
      " [ 98 155  98 128]\n",
      " [106 124 121 130]\n",
      " [127 139 108 109]\n",
      " [102 112 143 128]\n",
      " [123 103 147 114]\n",
      " [114 127 124 124]\n",
      " [130 123 110 128]\n",
      " [132 124 117 120]\n",
      " [137 125 136  97]]\n",
      "[1100 1149 1124 1078]\n",
      "The winner is agent: 3 with a score of 1078!\n"
     ]
    }
   ],
   "source": [
    "game.playHand()\n",
    "game.printResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed6b45-bad6-47c4-8ffd-4a748685a3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c60d6-23ba-405a-838b-9d7287e12ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e90e0c-49df-4744-9674-db3576c1b491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95802d-1e4a-4ee1-9317-ad76b49bbf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76ffab-72f1-42b4-9001-a81580abe7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cea7a-7362-439c-b7bb-a69876359b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185200c-2f97-4090-ac85-9963f39e0ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90842e4f-da9a-4db1-ba2d-79dbe9ee6a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d06e5-c109-4313-9ad3-aebffbef7b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e1c23-13bb-43d2-96f4-d9b8d7cd91b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d5185-14e4-4dfc-88ef-0d7b3525d099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33216c5-ea67-4dc0-b7bd-1d4f54f0a109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34394ca-afce-4130-86d8-518de762430d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf91de7-6d74-4ea7-bd4c-5957126a02f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ffb2790-9c12-4ed3-a650-a0163e9a3526",
   "metadata": {},
   "source": [
    "## Below this point I'm including some code blocks that make inspection of the gameplay and agent status easy... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ff4c26a2-a5a8-410d-9795-b88680257fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  25  57  20]\n",
      " [130  97 129 123]\n",
      " [136 104 106 135]\n",
      " [118 118 140 107]\n",
      " [ 83 136 124 142]\n",
      " [127 127 113 120]\n",
      " [133 114 120 122]\n",
      " [119 151 134  87]\n",
      " [122 132 116 123]\n",
      " [119 110 133 133]]\n",
      "[1087 1114 1172 1112]\n",
      "The winner is agent: 0 with a score of 1087!\n"
     ]
    }
   ],
   "source": [
    "numPlayers = 4\n",
    "highestDominoe = 9\n",
    "game = dg.dominoeGame(highestDominoe, agents=(da.doubleAgent, da.greedyAgent, None, da.stupidAgent))\n",
    "game.playGame()\n",
    "game.printResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6f8fa238-52ca-4b98-b287-1a9d283e1c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 9|7 ', ' 7|7 ', ' 7|5 ', ' 5|5 ', ' 5|4 ', ' 4|0 ', ' 0|0 ']\n",
      "[' 9|5 ', ' 5|6 ', ' 6|7 ', ' 7|8 ', ' 8|4 ', ' 4|7 ', ' 7|3 ', ' 3|8 ', ' 8|2 ', ' 2|4 ']\n",
      "[' 9|8 ', ' 8|0 ', ' 0|7 ', ' 7|1 ', ' 1|1 ', ' 1|5 ', ' 5|3 ', ' 3|2 ']\n",
      "[' 9|2 ', ' 2|6 ', ' 6|3 ', ' 3|0 ', ' 0|5 ', ' 5|8 ', ' 8|1 ']\n",
      "[' 9|6 ', ' 6|0 ', ' 0|9 ', ' 9|3 ', ' 3|3 ', ' 3|4 ', ' 4|4 ', ' 4|6 ', ' 6|6 ', ' 6|1 ', ' 1|4 ', ' 4|9 ', ' 9|1 ']\n"
     ]
    }
   ],
   "source": [
    "numPlayers = 4\n",
    "highestDominoe = 9\n",
    "game = dg.dominoeGame(highestDominoe, agents=(da.doubleAgent, da.greedyAgent, None, da.stupidAgent))\n",
    "game.playHand()\n",
    "df.gameSequenceToString(game.dominoes, game.lineSequence, game.linePlayDirection, player=None, playNumber=None) #player=game.linePlayer, playNumber=game.linePlayNumber)\n",
    "df.gameSequenceToString(game.dominoes, game.dummySequence, game.dummyPlayDirection, player=None, playNumber=None) #player=game.linePlayer, playNumber=game.linePlayNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "1d0ae873-32f9-4a21-9724-741e666c36b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 0: []\n",
      "line 1: []\n",
      "line 2: []\n",
      "line 3: []\n",
      "dummy: []\n",
      "Dominoe: None, Location: None\n"
     ]
    }
   ],
   "source": [
    "# Example of options list for current game (requires game.initializeHand() and game.presentGameState() to be run)\n",
    "lineOptions, dummyOptions = game.agents[0].playOptions()\n",
    "df.printDominoeList(lineOptions, game.agents[0].dominoes, name='line')\n",
    "df.printDominoeList(dummyOptions, game.agents[0].dominoes, name='dummy:')\n",
    "dominoe, location = game.agents[0].selectPlay()\n",
    "print(f\"Dominoe: {dominoe}, Location: {location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "81fa6014-2a23-40f7-9955-d7f4ee1dc486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [1], []]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# game details\n",
    "game.lineSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bb5b6-46c8-4c45-acfd-d4dd83f1c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random game states and then train a network to predict current hand score...\n",
    "def generateRandomHand(dominoes, numDominoes, numPlayers, verboseOutput=False):\n",
    "    numInHands = np.random.randint(numDominoes+1)\n",
    "    idxInHands = np.random.choice(numDominoes,numInHands,replace=False)\n",
    "    handIdx = np.random.randint(0, numPlayers, numInHands)\n",
    "    played = np.full(numDominoes, True)\n",
    "    myHand = np.full(numDominoes, False)\n",
    "    otherHands = np.full(numDominoes, False)\n",
    "    played[idxInHands]=False\n",
    "    myHand[idxInHands[handIdx==0]] = True\n",
    "    otherHands[idxInHands[handIdx>0]] = True\n",
    "    myHandValue = np.sum(dominoes[myHand])\n",
    "    otherHandValue = np.sum(dominoes[otherHands])\n",
    "    if verboseOutput:\n",
    "        return played, myHand, otherHands, myHandValue, otherHandValue\n",
    "    networkInput = (np.concatenate((played, myHand))*1)\n",
    "    networkOutput = np.array([myHandValue, otherHandValue])\n",
    "    # networkInput = np.concatenate((np.sum(dominoes.T * played, axis=0), np.sum(dominoes.T * myHand, axis=0)))\n",
    "    return networkInput, networkOutput\n",
    "\n",
    "# played, myHand, otherHands, myHandValue, otherHandValue = generateRandomHand(dominoes, numDominoes, numPlayers, verboseOutput=True)\n",
    "networkInput, networkOutput = generateRandomHand(dominoes, numDominoes, numPlayers)\n",
    "\n",
    "numPlayers = 2\n",
    "highestDominoe = 4\n",
    "numDominoes = df.numberDominoes(highestDominoe)\n",
    "dominoes = df.listDominoes(highestDominoe)\n",
    "\n",
    "handValueNetwork = dn.handValueNetwork(numPlayers,numDominoes,highestDominoe)\n",
    "handValueNetwork.to(device)\n",
    "\n",
    "numIterations = 10000\n",
    "target = np.zeros((numIterations, 2))\n",
    "pred = np.zeros((numIterations, 2))\n",
    "storeLoss = np.zeros(numIterations)\n",
    "\n",
    "lossFunction = nn.SmoothL1Loss()\n",
    "# optimizer = torch.optim.SGD(handValueNetwork.parameters(), lr=1e-2, momentum=0.5)\n",
    "optimizer = torch.optim.Adadelta(handValueNetwork.parameters())\n",
    "\n",
    "for ii in tqdm(range(numIterations)):\n",
    "    networkInput, networkOutput = generateRandomHand(dominoes, numDominoes, numPlayers)\n",
    "    target[ii] = np.copy(networkOutput)\n",
    "    \n",
    "    networkInput = torch.tensor(networkInput).to(device).float()\n",
    "    networkOutput = torch.tensor(networkOutput).to(device).float()\n",
    "    \n",
    "    output = handValueNetwork(networkInput)\n",
    "    \n",
    "    loss = lossFunction(output, networkOutput)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    storeLoss[ii] = loss.item()\n",
    "    pred[ii] = output.detach().cpu().numpy()\n",
    "    \n",
    "print(np.mean(storeLoss[-100:]))\n",
    "fig,ax = plt.subplots(1,2,figsize=(7,3))\n",
    "ax[0].plot(range(numIterations), storeLoss, linewidth=0.5)\n",
    "ax[1].plot(range(numIterations), pred[:,0]-target[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4ed9f-c61e-4e19-9235-6e2ec58a5b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80913481-af53-4d8a-92b8-7916242d3f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
